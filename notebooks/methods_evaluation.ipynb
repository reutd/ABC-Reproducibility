{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "!pip install scanpy==1.9.1\n",
        "!pip install matplotlib==3.6\n",
        "!pip install scib\n",
        "!pip install louvain\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# WHEN USING COLAB, PLEASE RESTART RUNTIME AFTER RUNNING THIS CELL\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# (some of the packages being used in this notebook are automatically loaded \n",
        "# before installation of a different version and they need to be reloaded)\n"
      ],
      "metadata": {
        "id": "cMPrgehUBk-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LISI scores use the knn_graph.o file created by the cpp code: knn_graph.cpp\n",
        "# In order to use these metrics we need to recompile the code in the current \n",
        "# environment, and replace the existing file with the compiled new file, \n",
        "# using the following code:\n",
        "\n",
        "!wget https://raw.githubusercontent.com/theislab/scib/main/scib/knn_graph/knn_graph.cpp\n",
        "!g++ -O3 -o knn_graph.o knn_graph.cpp\n",
        "\n",
        "import shutil\n",
        "import pathlib\n",
        "import scib\n",
        "import os\n",
        "\n",
        "root = pathlib.Path(scib.__file__).parent\n",
        "print(root)\n",
        "\n",
        "cpp_file_path = (\n",
        "    # root / \"knn_graph/knn_graph.o\"\n",
        "    root / \"knn_graph/\"\n",
        ")\n",
        "\n",
        "os.remove(str(root / \"knn_graph/knn_graph.o\"))\n",
        "shutil.move(\"knn_graph.o\", str(cpp_file_path))"
      ],
      "metadata": {
        "id": "Vi1lH1KAJy5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import numpy as np\n",
        "import random\n",
        "import scib as scIB\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/modules/')\n",
        "from evaluate_integration import evaluate_integration\n",
        "from datasets_dict import datasets\n",
        "\n",
        "# list of methods to evaluate\n",
        "methods = [\n",
        "            'scvi',\n",
        "            'scanvi',\n",
        "            'scanorama',\n",
        "            'combat',            \n",
        "            'Seurat',\n",
        "            'AutoClass',\n",
        "            'scgen',\n",
        "            'ABC',\n",
        "            ]\n",
        "\n",
        "# path to the original dataset (after subset to 3000 highly variable genes)\n",
        "orig_base_path = '/content/drive/MyDrive/Colab Notebooks/integrationDatasets/'\n",
        "\n",
        "# path to the metrics csv file\n",
        "metrics_path = os.path.join(orig_base_path, 'final_metrics')\n",
        "\n",
        "# create metrics folder if does not exist\n",
        "Path(metrics_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# which integration metrics to calculate (mark True to use the metric)\n",
        "eval_params = {\n",
        "    'silhouette_': True,\n",
        "    'nmi_': True,\n",
        "    'ari_': True,\n",
        "    'cell_cycle_': True,    # turns to false for ATAC\n",
        "    'isolated_labels_': True,\n",
        "    'hvg_score_': True,\n",
        "    'graph_conn_': True,\n",
        "    'lisi_graph_': True,\n",
        "    'trajectory_': True     # turns to false if pseudotime info is not present\n",
        "}\n",
        "\n",
        "\n",
        "# Evaluate each dataset in datasets\n",
        "# for dataset_name in datasets.keys():\n",
        "for dataset_name in ['small_atac_windows']:\n",
        "\n",
        "    # get dataset parameters from the dictionary\n",
        "    label_key = datasets[dataset_name]['label_key']\n",
        "    batch_key = datasets[dataset_name]['batch_key']\n",
        "    subsample = datasets[dataset_name]['subsample']\n",
        "    atac = datasets[dataset_name]['ATAC']\n",
        "    organism = datasets[dataset_name]['organism']\n",
        "\n",
        "    if atac:\n",
        "      data_type = 'ATAC'\n",
        "    else:\n",
        "      data_type = 'RNA'\n",
        "\n",
        "\n",
        "    # load original (unintegrated) dataset\n",
        "    orig_path = os.path.join(orig_base_path, f\"{dataset_name}_hvg.h5ad\")\n",
        "    orig_data = sc.read(orig_path)\n",
        "\n",
        "    # make sure the data matrix is not sparse\n",
        "    if scipy.sparse.issparse(orig_data.X):\n",
        "      print(\"The given adata.X matrix is sparse. Converting to dense.\")\n",
        "      orig_data.X = orig_data.X.todense()\n",
        "    \n",
        "    # # preview the dataset\n",
        "    # print(\"original dataset object:\", orig_data)\n",
        "\n",
        "\n",
        "    # evaluate each method for the current dataset\n",
        "    for method in methods:\n",
        "\n",
        "        print('-' * 50)\n",
        "        print(\"Using dataset: \", dataset_name)\n",
        "        print(\"Evaluating method: \", method)\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        seed_value = 1\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed_value)  # the hash seed for Python\n",
        "        random.seed(seed_value)  # the seed for Python's built-in random module\n",
        "        np.random.seed(seed_value)  # the seed for Numpy (which Scipy relies on)\n",
        "        tf.random.set_seed(seed_value)  # the seed for TensorFlow\n",
        "\n",
        "        \n",
        "\n",
        "        # load integrated dataset\n",
        "        integ_path = os.path.join(orig_base_path, \"integratedDatasets\",\n",
        "                                  method, f\"{dataset_name}_integrated.h5ad\")\n",
        "        integ_data = sc.read(integ_path)\n",
        "\n",
        "       \n",
        "        # make sure data is not sparse\n",
        "        if scipy.sparse.issparse(integ_data.X):\n",
        "          print(\"The given integ_data.X matrix is sparse. Converting to dense.\")\n",
        "          integ_data.X = integ_data.X.todense()\n",
        "       \n",
        "\n",
        "        # fix gene names for Seurat naming convention\n",
        "        if method == 'Seurat':\n",
        "\n",
        "            # if the var names contain '_' instead of \"-\" - use the next lines to convert\n",
        "            integ_data.var_names = [id.replace('-', '_') for id in integ_data.var_names]\n",
        "            orig_data.var_names = [id.replace('-', '_') for id in orig_data.var_names]\n",
        "        \n",
        "            # only include cells from Seurat integration\n",
        "            orig_data = orig_data[:, integ_data.var_names]\n",
        "        \n",
        "\n",
        "\n",
        "        # --- Evaluate integration ---\n",
        "        print(\"calculating integration scores:\")\n",
        "        integ_scores = pd.DataFrame()\n",
        "\n",
        "        try:\n",
        "            integ_scores = evaluate_integration(orig_data, integ_data,\n",
        "                                                eval_params,\n",
        "                                                batch_key=batch_key,\n",
        "                                                label_key=label_key,\n",
        "                                                data_type=data_type,\n",
        "                                                organism=organism,\n",
        "                                                bio_con_w=0.5)\n",
        "        except Exception as e:\n",
        "            print(f'---exception when evaluating integration of {dataset_name} for {method}! moving on...')\n",
        "            print(e)\n",
        "            \n",
        "            continue\n",
        "\n",
        "        print(\"integration scores:\")\n",
        "        print(integ_scores)\n",
        "\n",
        "\n",
        "        # --- save scores ---        \n",
        "        # Transpose the DataFrame and drop Nan columns\n",
        "        integ_scores = integ_scores.T\n",
        "        integ_scores = integ_scores.dropna(axis=1, how='all')\n",
        "\n",
        "        # Rename the index of the transposed DataFrame with the value of the method\n",
        "        integ_scores.index = [method]\n",
        "\n",
        "        # define metrics output path\n",
        "        metrics_file = os.path.join(metrics_path, f'{dataset_name}_metrics.csv')\n",
        "\n",
        "        # save the scores dataframe to a csv file\n",
        "        if os.path.exists(metrics_file):\n",
        "            integ_scores.to_csv(metrics_file, mode='a', index=True, header=False)\n",
        "        else:\n",
        "            integ_scores.to_csv(metrics_file, index=True)\n"
      ],
      "metadata": {
        "id": "-rIMgQclPG41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}