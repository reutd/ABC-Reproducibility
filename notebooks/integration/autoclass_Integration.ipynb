{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy==1.9.1\n",
        "!pip install matplotlib==3.6\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install numpy pandas sklearn matplotlib\n",
        "!git clone https://github.com/datapplab/AutoClass"
      ],
      "metadata": {
        "id": "kSAiJVPK87Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvp-ZZbxP-1I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import scanpy as sc\n",
        "import time\n",
        "import sys\n",
        "from AutoClass.AutoClass.AutoClass import AutoClassImpute, take_norm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# When using colab, set the path to the modules directory to use saved modules\n",
        "sys.path.append('/content/drive/MyDrive/modules/')\n",
        "from datasets_dict import datasets\n",
        "\n",
        "# path to the original dataset (after subset to 3000 highly variable genes)\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/integrationDatasets/'\n",
        "execution_times = {}\n",
        "\n",
        "\n",
        "# for dataset_name in datasets.keys():\n",
        "for dataset_name in ['small_atac_windows']:\n",
        "\n",
        "  # get dataset parameters\n",
        "  label_key = datasets[dataset_name]['label_key']\n",
        "  batch_key = datasets[dataset_name]['batch_key']\n",
        "\n",
        "  # set paths\n",
        "  inPath = os.path.join(base_path, f\"{dataset_name}_hvg.h5ad\")\n",
        "  outPath = os.path.join(base_path, 'integratedDatasets', 'AutoClass')\n",
        "  \n",
        "  # create directory if does not exists\n",
        "  Path(outPath).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # read the original dataset\n",
        "  adata = sc.read(inPath)\n",
        "  cell_type_labels = adata.obs[label_key]\n",
        "\n",
        "  # integrate the dataset\n",
        "  start_time = time.time()\n",
        "  res = AutoClassImpute(adata.X,cellwise_norm=False,log1p=False,\n",
        "                        truelabel=cell_type_labels)\n",
        "  adata.X = res['imp']\n",
        "  end_time = time.time()\n",
        "\n",
        "  # save integration duration time\n",
        "  elapsed_time = end_time - start_time\n",
        "  minutes, seconds = divmod(elapsed_time, 60)\n",
        "  execution_times[dataset_name] = elapsed_time\n",
        "\n",
        "  print(\"Integrated: \", dataset_name)\n",
        "  print(f\"Duration: {minutes} minutes and {seconds} seconds\")\n",
        "  \n",
        "  # write integrated data\n",
        "  sc.write(os.path.join(outPath, f\"{dataset_name}_integrated.h5ad\"), adata)\n",
        "  print(\"Integrated data saved\")\n",
        "\n",
        "# write execution times data\n",
        "df = pd.DataFrame(list(execution_times.items()), columns=['Dataset', 'Execution Time'])\n",
        "df.to_csv(os.path.join(outPath, 'execution_times.csv'), index=False)\n"
      ]
    }
  ]
}