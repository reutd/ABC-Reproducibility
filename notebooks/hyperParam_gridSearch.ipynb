{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wEyBYUj_ln9-S_cqYPy5rFe6f6NOMq3x","timestamp":1684330203043},{"file_id":"1sNfIbcLJfvha_Wx1T53hInASKhrEoWfV","timestamp":1684167118090},{"file_id":"1knJPo_KRy613p5bjyD1_r-YWhhlqJQAi","timestamp":1684060867993},{"file_id":"1ZorHPTADBsyg8WOlzk6iBoN_gN0hDnee","timestamp":1683533089412},{"file_id":"1gBgf2mlEqX-W5mkHGd_kZKskkAtUZo7g","timestamp":1683458140797}],"gpuType":"V100","machine_shape":"hm","mount_file_id":"1nLtOhyli7YjLCfFGbJ4tnjtHnVT9LOCm","authorship_tag":"ABX9TyO24rWKj3H5b9Zze5AkjaRe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install git+https://github.com/reutd/ABC.git\n","!pip install scanpy==1.9.1\n","!pip install matplotlib==3.6\n","!pip install scib\n","!pip install louvain"],"metadata":{"id":"ilDQwld2pCyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LISI scores use the knn_graph.o file created by the cpp code: knn_graph.cpp\n","# In order to use these metrics we need to recompile the code in the current \n","# environment, and replace the existing file with the compiled new file, \n","# using the following code:\n","\n","!wget https://raw.githubusercontent.com/theislab/scib/main/scib/knn_graph/knn_graph.cpp\n","!g++ -O3 -o knn_graph.o knn_graph.cpp\n","\n","import shutil\n","import pathlib\n","import scib\n","import os\n","\n","root = pathlib.Path(scib.__file__).parent\n","print(root)\n","\n","cpp_file_path = (root / \"knn_graph/\")\n","\n","os.remove(str(root / \"knn_graph/knn_graph.o\"))\n","shutil.move(\"knn_graph.o\", str(cpp_file_path))"],"metadata":{"id":"yEZkiNpF6Lsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rvp-ZZbxP-1I"},"outputs":[],"source":["import os\n","from pathlib import Path\n","import scanpy as sc\n","import itertools as it\n","import time\n","import traceback\n","import sys\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from ABC import ABC\n","\n","sys.path.append('/content/drive/MyDrive/modules/')\n","from evaluate_integration import evaluate_integration\n","\n","\n","\n","# integrates the data and evaluates the integration. Returns the final score,\n","# the metrics dataframe (row per metric) and the trained model.\n","def integrate_and_score(orig_dataset, eval_params, dataset_params, \n","                        training_hParams, verbos=True):\n","\n","  label_key = dataset_params['label_key']\n","  batch_key = dataset_params['batch_key']\n","  organism = dataset_params['organism']\n","  \n","  # --- Data Integration ---\n","  lr = training_hParams['LR']\n","  rec_l = training_hParams['Rec_Loss_W']\n","  l_dim = training_hParams['L_Dim']\n","\n","  # create the model\n","  start_time = time.time()\n","  model = ABC(orig_dataset, batch_key, label_key, latent_dim=l_dim)\n","\n","  # train the model and integrate the dataset\n","  integrated = model.batch_correction(data_name=dataset_name,                                                                             \n","                                      base_LR=lr,\n","                                      recon_loss_w=rec_l,\n","                                      )\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","  minutes, seconds = divmod(elapsed_time, 60)\n","\n","  print(\"Integrated: \", dataset_name)\n","  print(f\"Duration: {minutes} minutes and {seconds} seconds\")\n","\n","\n","  # --- Integration Evaluation ---\n","  if dataset_params['ATAC']:\n","    data_type = 'ATAC'\n","  else:\n","    data_type = 'RNA'\n","\n","\n","  metrics = evaluate_integration(orig_dataset, integrated, eval_params,\n","                                 batch_key=batch_key,\n","                                  label_key=label_key,\n","                                  data_type=data_type,\n","                                  organism=organism,\n","                                 )\n","\n","  \n","  # store execution time\n","  metrics.loc['Time'] = elapsed_time\n","\n","  \n","  if verbos:\n","    print(\"integration scores:\")\n","    print(metrics)\n","\n","  final_score = metrics.loc['Final_Score'][0]\n","\n","  return final_score, metrics, model, integrated\n","\n","\n","# datasets to be used for hyperparameter optimization\n","datasets = {\n","    'small_atac_gene_activity':\n","        {\n","            'label_key': 'final_cell_label',\n","            'batch_key': 'batchname',\n","            'organism': 'mouse',\n","            'subsample': 1,\n","            'ATAC': True,\n","         },\n","\n","    'human_pancreas_norm_complexBatch':\n","        {\n","            'label_key': 'celltype',\n","            'batch_key': 'tech',\n","            'organism': 'human',\n","            'ATAC': False,\n","            'subsample': 1,\n","        },\n","}\n","\n","# evaluation metrics to calculate\n","eval_params = {\n","    'silhouette_': True,\n","    'nmi_': True,\n","    'ari_': True,\n","    'cell_cycle_': True,    # turns to false for ATAC\n","    'isolated_labels_': True,\n","    'hvg_score_': True,\n","    'graph_conn_': True,\n","    'lisi_graph_': True,\n","    'trajectory_': True     # turns to false if pseudotime info not present\n","}\n","\n","\n","# Hyperparameters to test\n","param_grid = {\n","    'LR': [None, 0.001, 0.0001, 0.0002],\n","    'L_Dim': [32, 64, 128, 256, 512, 1024],\n","    'Rec_Loss_W': [0.2, 0.4, 0.6, 0.8]\n","}\n","\n","# define paths\n","base_path = '/content/drive/MyDrive/Colab Notebooks/integrationDatasets/'\n","metrics_path = os.path.join(base_path, 'final_metrics')\n","weights_path = os.path.join(base_path, 'trainedModels')\n","\n","\n","# create directories if needed\n","Path(metrics_path).mkdir(parents=True, exist_ok=True)\n","Path(weights_path).mkdir(parents=True, exist_ok=True)\n","\n","# save lists of failed param combinations\n","failed_params={\n","    'small_atac_gene_activity': [],\n","    'human_pancreas_norm_complexBatch': [],\n","}\n","\n","subsample = False\n","best_score = 0\n","\n","# Run hyperparameter grid search for each dataset\n","for dataset_name in datasets.keys():\n","\n","  # get dataset parameters\n","  dataset_params = datasets[dataset_name]\n","\n","  # read data\n","  inPath = os.path.join(base_path, f\"{dataset_name}_hvg.h5ad\")\n","  adata = sc.read(inPath)\n","\n","  if subsample:\n","    adata = sc.pp.subsample(adata, n_obs=1001, random_state=1, copy=True)\n","\n","    \n","  # Generate all combinations\n","  all_names = param_grid.keys()\n","  combinations = it.product(*(param_grid[name] for name in all_names))\n","  \n","  # For each combination, train and evaluate the model\n","  for combination in combinations:\n","\n","    params = dict(zip(all_names, combination))\n","\n","    # display current combination and dataset name\n","    print('-' * 100)\n","    print(f'Using params: {params} with dataset: {dataset_name}')\n","\n","\n","    # run integration and score results\n","    try:\n","      score, metrics, model, integrated = integrate_and_score(adata, eval_params = eval_params, \n","                                              dataset_params = dataset_params, \n","                                              training_hParams = params)\n","    except Exception as e:\n","      print('-' * 100)\n","      print(f'Failed to integrate or evaluate params: {params} with dataset: {dataset_name}')\n","      # print(e)\n","      traceback.print_exc(limit=None)\n","      failed_params[dataset_name].append(params)\n","      print(\"moving on...\")\n","      print('-' * 100)\n","      \n","      continue\n","\n","\n","    # display parameters and final score\n","    print(f'Params: {params} Final Score: {score}')\n","    \n","    # process and save metrics\n","    metrics_file = os.path.join(metrics_path, f'{dataset_name}_hyperParamOpt.csv')\n","    metrics = metrics.T\n","    metrics = metrics.dropna(axis=1, how='all')\n","\n","    # Rename the index of the transposed DataFrame\n","    metrics.index = [str(params)]\n","\n","    if os.path.exists(metrics_file):\n","      metrics.to_csv(metrics_file, mode='a', index=True, header=False)\n","    else:\n","      metrics.to_csv(metrics_file, index=True)\n","\n","\n","    # save trained model weights and update best score\n","    if score > best_score:\n","      best_score = score\n","      print(f'new best score: {score}')\n","      print(f'for combination: {params}')\n","      \n","      # save model weights       \n","      weights_filepath = os.path.join(weights_path, dataset_name, \n","                                      f'{dataset_name}_bestModel')\n","      model.save_weights(weights_filepath, overwrite=True)\n","\n","\n","print(\"Failed runs:\")\n","print(failed_params)\n"]}]}